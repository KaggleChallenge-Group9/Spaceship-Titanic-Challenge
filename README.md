# ğŸš€ Spaceship Titanic Challenge â€“ Kaggle Group Assignment

**Team Name:** Cosmic Learners (Group 9)  
**Challenge:** [Spaceship Titanic - Kaggle Competition](https://www.kaggle.com/competitions/spaceship-titanic)  
**Course Assignment:** Data Science Assignment 3 â€“ Group Kaggle Challenge  
**Google Colab:** [Google Colab Link](https://colab.research.google.com/drive/1hz_5xvlN9PqSKk90i8JNYxFYfH7eqqBH)
**GitHub Repo:** [Github Link](https://github.com/KaggleChallenge-Group9/Spaceship-Titanic-Challenge)
  

---
## ğŸ‘¨â€ğŸš€ Project Summary

Welcome to our repository for the **Spaceship Titanic Challenge**, a machine learning competition hosted on Kaggle. Our objective was to develop predictive models that determine whether a passenger was **transported to an alternate dimension** during a futuristic interstellar voyage.

Our group, **Cosmic Learners**, collaboratively built a complete and reproducible machine learning pipeline, including exploratory analysis, feature engineering, model training, hyperparameter tuning, and performance evaluation.

---

## ğŸ‘¥ Team Members

- Ayisha Fidha Maniyodan  
- Diya Amith Kodappully  
- Dona Uresha Pamodi Dasanayake  
- Fawas Afsal  
- Mohammed Nihad Kaipalli  
- Sam Jacob  
- Sandra Binu  
- Sharon Zacharia  

---

## ğŸ§  Project Workflow

This notebook showcases a comprehensive machine learning pipeline divided into the following structured sections:

### 1. ğŸ§­ Exploratory Data Analysis (EDA)
- Univariate & bivariate feature exploration  
- Correlation analysis  
- Visualizations using seaborn, matplotlib, squarify, and pie charts  
- Identification of missing values and patterns

### 2. ğŸ§¹ Data Preprocessing
- Missing value imputation (mean/mode strategy)  
- Feature extraction from `Cabin`, `Name`, and other composite fields  
- Label encoding for categorical variables  
- Standardization of numerical features

### 3. ğŸ§ª Model Training & Evaluation
- Logistic Regression  
- Random Forest  
- XGBoost  
- Deep Neural Network (Keras Sequential Model)  
- Evaluation metrics: Accuracy, Precision, Recall, F1 Score, AUC-ROC  
- ROC Curves & Model Comparison Charts

### 4. ğŸ” Hyperparameter Tuning
- `GridSearchCV` for Logistic Regression, Random Forest, and XGBoost  
- EarlyStopping & ReduceLROnPlateau for DNNs  
- PCA used optionally for dimensionality reduction & visualization

### 5. ğŸ“ˆ Explainability
- Model explainability with SHAP (SHapley Additive exPlanations)  
- Visual insights into feature contributions for top predictions

### 6. ğŸ“¤ Submission Generation
- Final prediction pipeline applied to the test set  
- Submission file generated in proper Kaggle format

---

## ğŸ§‘â€ğŸ’» Collaboration & Tools

- **Version Control:** GitHub (branching, pull requests, versioning)
- **Shared Environment:** Google Colab for code collaboration
- **Libraries:** Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, XGBoost, TensorFlow/Keras, SHAP

---

